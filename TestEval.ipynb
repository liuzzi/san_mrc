{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import msgpack\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from src.model import DocReaderModel\n",
    "from src.batcher import load_meta_with_vocab, BatchGen\n",
    "from my_utils.data_utils import feature_func_eval\n",
    "from my_utils.tokenizer import Vocabulary, reform_text\n",
    "from config import set_args\n",
    "from my_utils.utils import set_environment\n",
    "from my_utils.log_wrapper import create_logger\n",
    "from my_utils.squad_eval import evaluate\n",
    "from my_utils.data_utils import predict_squad, gen_name, load_squad_v2_label, compute_acc\n",
    "from my_utils.squad_eval_v2 import *\n",
    "import sys\n",
    "print(torch.cuda.is_available())\n",
    "from my_utils.data_utils import predict_squad, gen_name, gen_gold_name, load_squad_v2_label, compute_acc\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "loading as cuda enabled\n",
      "Done loading model and resources\n"
     ]
    }
   ],
   "source": [
    "sys.argv = ['train.py']\n",
    "version = 'v2'\n",
    "\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 20:22 cp_epoch_7_em_71_f1_72.pt\n",
    "# -rwxrwxrwx 1 root root 9.0K Feb 20 20:22 dev_output_7.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 19:52 cp_epoch_6_em_78_f1_78.pt\n",
    "# -rwxrwxrwx 1 root root 7.6K Feb 20 19:51 dev_output_6.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 19:20 cp_epoch_5_em_70_f1_71.pt\n",
    "# -rwxrwxrwx 1 root root  11K Feb 20 19:20 dev_output_5.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 18:47 cp_epoch_4_em_74_f1_75.pt\n",
    "# -rwxrwxrwx 1 root root 8.0K Feb 20 18:47 dev_output_4.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 18:15 cp_epoch_3_em_76_f1_76.pt\n",
    "# -rwxrwxrwx 1 root root 8.5K Feb 20 18:14 dev_output_3.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 17:40 cp_epoch_2_em_80_f1_80.pt\n",
    "# -rwxrwxrwx 1 root root 7.0K Feb 20 17:40 dev_output_2.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 17:05 cp_epoch_1_em_71_f1_72.pt\n",
    "# -rwxrwxrwx 1 root root  10K Feb 20 17:05 dev_output_1.json\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 16:29 best_epoch_0_em_81_f1_81.pt\n",
    "# -rwxrwxrwx 1 root root 1.2G Feb 20 16:29 cp_epoch_0_em_81_f1_81.pt\n",
    "# -rwxrwxrwx 1 root root 6.9K Feb 20 16:29 dev_output_0.json\n",
    "\n",
    "\n",
    "model_filepath = 'checkpoint/cp_epoch_6_em_76_f1_76.pt'\n",
    "args = set_args()\n",
    "\n",
    "opt = vars(args)\n",
    "print(args.cuda)\n",
    "\n",
    "embedding, opt, vocab = load_meta_with_vocab(opt, gen_name(args.data_dir, args.meta, version, suffix='pick'))\n",
    "\n",
    "# print(opt)\n",
    "model = DocReaderModel.load(model_filepath,embedding,gpu=args.cuda)\n",
    "\n",
    "\n",
    "model.setup_eval_embed(embedding)\n",
    "\n",
    "if args.cuda:\n",
    "    print('loading as cuda enabled')\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "\n",
    "with open(os.path.join('resource', 'vocab_tag.pick'),'rb') as f:\n",
    "    vocab_tag = pickle.load(f)\n",
    "with open(os.path.join('resource','vocab_ner.pick'),'rb') as f:\n",
    "    vocab_ner = pickle.load(f)\n",
    "    \n",
    "NLP = spacy.load('en')\n",
    "print('Done loading model and resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding( m, threshold=5):\n",
    "    encoding = np.ones((m, m), dtype=np.float32)\n",
    "    for i in range(m):\n",
    "        for j in range(i, m):\n",
    "            if j - i > threshold:\n",
    "                encoding[i][j] = float(1.0 / math.log(j - i + 1))\n",
    "    return torch.from_numpy(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_squad(model, data, v2_on=False):\n",
    "    data.reset()\n",
    "    span_predictions = {}\n",
    "    label_predictions = {}\n",
    "    for batch in data:\n",
    "        phrase, spans, scores = model.predict(batch)\n",
    "        uids = batch['uids']\n",
    "        for uid, pred in zip(uids, phrase):\n",
    "            span_predictions[uid] = pred\n",
    "        if v2_on:\n",
    "            for uid, pred in zip(uids, scores):\n",
    "                label_predictions[uid] = pred\n",
    "    return span_predictions, label_predictions\n",
    "\n",
    "\n",
    "def load_squad(data_path):\n",
    "    with open(data_path) as dataset_file:\n",
    "        dataset_json = json.load(dataset_file)\n",
    "        dataset = dataset_json['data']\n",
    "        return dataset\n",
    "    \n",
    "def make_qid_to_has_ans(dataset):\n",
    "    qid_to_has_ans = {}\n",
    "    for article in dataset:\n",
    "        for p in article['paragraphs']:\n",
    "            for qa in p['qas']:\n",
    "                qid_to_has_ans[qa['id']] = bool(qa['answers'])\n",
    "    return qid_to_has_ans\n",
    "\n",
    "\n",
    "\n",
    "def get_raw_scores(dataset, preds):\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "    for article in dataset:\n",
    "        for p in article['paragraphs']:\n",
    "            for qa in p['qas']:\n",
    "                qid = qa['id']\n",
    "                gold_answers = [a['text'] for a in qa['answers']\n",
    "                                                if normalize_answer(a['text'])]\n",
    "                if not gold_answers:\n",
    "                    # For unanswerable questions, only correct answer is empty string\n",
    "                    gold_answers = ['']\n",
    "                if qid not in preds:\n",
    "                    # print('Missing prediction for %s' % qid)\n",
    "                    continue\n",
    "                a_pred = preds[qid]\n",
    "                # Take max over all gold answers\n",
    "                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
    "                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
    "    return exact_scores, f1_scores\n",
    "    \n",
    "    \n",
    "    \n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
    "    if not qid_list:\n",
    "        total = len(exact_scores)\n",
    "        return collections.OrderedDict([\n",
    "                ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
    "                ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
    "                ('total', total),\n",
    "        ])\n",
    "    else:\n",
    "        total = len(qid_list)\n",
    "\n",
    "    return collections.OrderedDict([\n",
    "            ('exact', 100.0 * sum(exact_scores[k] for k in f1_scores if k in qid_list) / total),\n",
    "            ('f1', 100.0 * sum(f1_scores[k] for k in f1_scores if k in qid_list) / total),\n",
    "            ('total', total),\n",
    "    ])\n",
    "\n",
    "def my_evaluation(dataset, preds, na_probs=None, na_prob_thresh=1.0):\n",
    "    has_na_prob_score = False if na_probs is None else True\n",
    "    if na_probs is None:\n",
    "            na_probs = {k: 0.0 for k in preds}\n",
    "    qid_to_has_ans = make_qid_to_has_ans(dataset)    # maps qid to True/False\n",
    "    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "    \n",
    "#     print('number of hasans:',len(has_ans_qids))\n",
    "#     print('number of noans:',len(no_ans_qids))\n",
    "#     print('len of preds:',len(preds))\n",
    "    \n",
    "    \n",
    "    #print golds\n",
    "#     print([dataset[list(preds.keys()).index(k)]['paragraphs'][0]['qas'][0]['question'] for k,v in preds.items() ])\n",
    "\n",
    "    exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "#     print('exact/f1 raw',exact_raw,f1_raw)\n",
    "    \n",
    "    exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans, na_prob_thresh)\n",
    "    f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans, na_prob_thresh)\n",
    "    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "    \n",
    "    if has_ans_qids:\n",
    "            has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=[k for k in f1_thresh if k in has_ans_qids])\n",
    "#             print('has_ans_eval',has_ans_eval)\n",
    "            merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
    "    if no_ans_qids:\n",
    "            no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=[k for k in f1_thresh if k in no_ans_qids])\n",
    "#             print('no_ans_eval',no_ans_eval)\n",
    "#             print(len(exact_thresh))\n",
    "            merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
    "    if has_na_prob_score:\n",
    "            find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n",
    "    return out_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 501 samples out of 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 64215/64215 [00:00<00:00, 937594.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('exact', 76.24750499001996), ('f1', 76.57645747466107), ('total', 501), ('HasAns_exact', 5.4945054945054945), ('HasAns_f1', 7.305551591265877), ('HasAns_total', 91), ('NoAns_exact', 91.95121951219512), ('NoAns_f1', 91.95121951219512), ('NoAns_total', 410)])\n"
     ]
    }
   ],
   "source": [
    "dev_data = BatchGen(gen_name(args.data_dir, args.dev_data, version),\n",
    "                      batch_size=8,\n",
    "                      gpu=False, is_train=False)\n",
    "\n",
    "results, labels = predict_squad(model,dev_data,v2_on=True)\n",
    "\n",
    "dev_labels = load_squad_v2_label('/media/frankie/Data/data/msmarco/msmarco_squad_dev.json')\n",
    "dev_gold = load_squad('/media/frankie/Data/data/msmarco/msmarco_squad_dev.json')\n",
    "metric = my_evaluation(dev_gold, results, na_prob_thresh=.5)\n",
    "\n",
    "em, f1 = metric['exact'], metric['f1']\n",
    "acc = compute_acc(labels, dev_labels)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'context':\"Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, and the HomePod smart speaker. Apple's software includes the macOS and iOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode. Its online services include the iTunes Store, the iOS App Store and Mac App Store, Apple Music, and iCloud. Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell Wozniak's Apple I personal computer. It was incorporated as Apple Computer, Inc., in January 1977, and said to be a tech unicorn.\",\n",
    "         'question':'Who is frankie liuzzi?',\n",
    "         'uid':'n2iu2b342'},\n",
    "       {'context':'The second context is called Secondarial.  The third is not.',\n",
    "       'question':'What is the second context called?',\n",
    "       'uid':'zus3bui2'}]\n",
    "\n",
    "\n",
    "v2_on = True\n",
    "is_train = False\n",
    "passages = [reform_text(sample['context']) for sample in data]\n",
    "passage_tokened = [doc for doc in NLP.pipe(passages, batch_size=1000, n_threads=3)]\n",
    "\n",
    "question_list = [reform_text(sample['question']) for sample in data]\n",
    "question_tokened = [question for question in NLP.pipe(question_list, batch_size=1000, n_threads=3)]\n",
    "\n",
    "\n",
    "generated_data = []\n",
    "\n",
    "for idx, sample in enumerate(data):\n",
    "    feat_dict = feature_func_eval(sample, question_tokened[idx], passage_tokened[idx], vocab, vocab_tag, vocab_ner, is_train, v2_on)\n",
    "    generated_data.append(feat_dict)\n",
    "\n",
    "pred_data = BatchGen( None,\n",
    "                  batch_size=args.batch_size,\n",
    "                  gpu=False, \n",
    "                 is_train=False, \n",
    "                 data_json=generated_data)\n",
    "\n",
    "\n",
    "top_k = 1\n",
    "for batch in pred_data:\n",
    "    start,end,lab = model.predict_eval(batch)\n",
    "\n",
    "    max_len = model.opt['max_len'] or start.size(1)\n",
    "    doc_len = start.size(1)\n",
    "    pos_enc = model.position_encoding(doc_len, max_len)\n",
    "    \n",
    "    for i,r in enumerate(lab):\n",
    "        scores = torch.ger(start[i], end[i])\n",
    "        scores = scores * pos_enc\n",
    "        scores.triu_()\n",
    "        scores = scores.numpy()\n",
    "        \n",
    "        label_score = float(lab[i])\n",
    "\n",
    "        \n",
    "        for k in range(1,top_k+1):\n",
    "            print()\n",
    "            best_idx = np.argpartition(scores, -k, axis=None)[-k]\n",
    "            best_score = np.partition(scores, -k, axis=None)[-k]\n",
    "            s_idx, e_idx = np.unravel_index(best_idx, scores.shape)\n",
    "            \n",
    "\n",
    "            beginning_index = 0\n",
    "            for z in range(s_idx-1,0,-1):\n",
    "                \n",
    "                cur_tok = passage_tokened[i][z].text\n",
    "                \n",
    "\n",
    "                if cur_tok == '.' or cur_tok == '!' or cur_tok == '?' or cur_tok == '\\n' or cur_tok == ']':\n",
    "                    beginning_index = z+1\n",
    "                    break\n",
    "\n",
    "            end_index = len(passage_tokened[i])\n",
    "            for z in range(e_idx,len(passage_tokened[i])):\n",
    "                \n",
    "                cur_tok = passage_tokened[i][z].text\n",
    "                \n",
    "                \n",
    "                if cur_tok == '.' or cur_tok == '!' or cur_tok == '?' or cur_tok == '\\n' or cur_tok == '[':\n",
    "                    end_index = z+1\n",
    "                    break\n",
    "\n",
    "            snippet = passage_tokened[i][beginning_index:end_index]\n",
    "\n",
    "\n",
    "            if label_score > .5:\n",
    "                print('No Answer :',best_score,label_score)\n",
    "    #             print(passage_tokened[i][s_idx:e_idx+1], best_score, label_score)\n",
    "            else:\n",
    "                print(passage_tokened[i][s_idx:e_idx+1].text, best_score, label_score)\n",
    "                print(snippet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_tokened[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = NLP('My name is Donald Trump and I work at Google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in doc:\n",
    "    print(w.ent_type_, w.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_func(question, context):\n",
    "    counter = Counter(w.text.lower() for w in context)\n",
    "    total = sum(counter.values())\n",
    "    freq = [counter[w.text.lower()] / total for w in context]\n",
    "    question_word = {w.text for w in question}\n",
    "    question_lower = {w.text.lower() for w in question}\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in question}\n",
    "    match_origin = [1 if w in question_word else 0 for w in context]\n",
    "    match_lower = [1 if w.text.lower() in question_lower else 0 for w in context]\n",
    "    match_lemma = [1 if (w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma else 0 for w in context]\n",
    "    features = np.asarray([freq, match_origin, match_lower, match_lemma], dtype=np.float32).T.tolist()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_func(NLP('What is the president?'),NLP('What the president is Donald Trump and he was a man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3.6",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
