{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import msgpack\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from src.model import DocReaderModel\n",
    "from src.batcher import load_meta_with_vocab, BatchGen\n",
    "from my_utils.data_utils import feature_func_eval\n",
    "from my_utils.tokenizer import Vocabulary, reform_text\n",
    "from config import set_args\n",
    "from my_utils.utils import set_environment\n",
    "from my_utils.log_wrapper import create_logger\n",
    "from my_utils.squad_eval import evaluate\n",
    "from my_utils.data_utils import predict_squad, gen_name, load_squad_v2_label, compute_acc\n",
    "from my_utils.squad_eval_v2 import my_evaluation as evaluate_v2\n",
    "import sys\n",
    "print(torch.cuda.is_available())\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "{'v2_on': False, 'log_file': 'san.log', 'data_dir': 'data/', 'meta': 'meta', 'train_data': 'train_data', 'dev_data': 'dev_data', 'dev_gold': 'dev', 'test_data': 'test_data', 'test_gold': 'test', 'covec_path': 'data/MT-LSTM.pt', 'glove': 'data/glove.840B.300d.txt', 'sort_all': False, 'threads': 8, 'vocab_size': 90953, 'covec_on': True, 'embedding_dim': 300, 'fasttext_on': False, 'pos_on': True, 'pos_vocab_size': 54, 'pos_dim': 12, 'ner_on': True, 'ner_vocab_size': 41, 'ner_dim': 8, 'feat_on': True, 'num_features': 4, 'prealign_on': True, 'prealign_head': 1, 'prealign_att_dropout': 0, 'prealign_norm_on': False, 'prealign_proj_on': False, 'prealign_bidi': False, 'prealign_hidden_size': 300, 'prealign_share': True, 'prealign_residual_on': False, 'prealign_scale_on': True, 'prealign_sim_func': 'dotproductproject', 'prealign_activation': 'relu', 'pwnn_on': True, 'pwnn_hidden_size': 300, 'contextual_hidden_size': 300, 'contextual_cell_type': 'lstm', 'contextual_weight_norm_on': False, 'contextual_maxout_on': False, 'contextual_residual_on': False, 'contextual_encoder_share': False, 'contextual_num_layers': 2, 'msum_hidden_size': 300, 'msum_cell_type': 'lstm', 'msum_weight_norm_on': False, 'msum_maxout_on': False, 'msum_residual_on': False, 'msum_lexicon_input_on': False, 'msum_num_layers': 1, 'deep_att_lexicon_input_on': True, 'deep_att_hidden_size': 300, 'deep_att_sim_func': 'dotproductproject', 'deep_att_activation': 'relu', 'deep_att_norm_on': True, 'deep_att_proj_on': False, 'deep_att_residual_on': False, 'deep_att_share': True, 'deep_att_opt': 0, 'self_attention_on': True, 'self_att_hidden_size': 300, 'self_att_sim_func': 'dotproductproject', 'self_att_activation': 'relu', 'self_att_norm_on': False, 'self_att_proj_on': False, 'self_att_residual_on': False, 'self_att_dropout': 0.1, 'self_att_drop_diagonal': True, 'self_att_share': True, 'query_sum_att_type': 'linear', 'query_sum_norm_on': False, 'max_len': 5, 'decoder_num_turn': 8, 'decoder_mem_type': 1, 'decoder_mem_drop_p': 0.1, 'decoder_opt': 0, 'decoder_att_hidden_size': 300, 'decoder_att_type': 'bilinear', 'decoder_rnn_type': 'gru', 'decoder_sum_att_type': 'bilinear', 'decoder_weight_norm_on': False, 'classifier_merge_opt': 0, 'classifier_dropout_p': 0.4, 'classifier_weight_norm_on': True, 'classifier_gamma': 1, 'classifier_threshold': 0.5, 'label_size': 1, 'elmo_on': False, 'elmo_config_path': 'data/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json', 'elmo_weight_path': 'data/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5', 'elmo_size': 1024, 'elmo_lexicon_on': False, 'elmo_att_on': True, 'elmo_self_att_on': False, 'elmo_l2': 0.001, 'elmo_dropout': 0.5, 'cuda': False, 'log_per_updates': 100, 'epoches': 50, 'batch_size': 32, 'optimizer': 'adamax', 'grad_clipping': 5, 'weight_decay': 0, 'learning_rate': 0.002, 'momentum': 0, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_emb': 0.4, 'dropout_cov': 0.4, 'dropout_w': 0.05, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'fix_embeddings': False, 'tune_partial': 1000, 'model_dir': 'checkpoint', 'seed': 2018, 'resume': ''}\n",
      "Done loading model and resources\n"
     ]
    }
   ],
   "source": [
    "sys.argv = ['train.py']\n",
    "version = 'v2'\n",
    "model_filepath = 'resource/em_67_f1_69.pt'\n",
    "args = set_args()\n",
    "\n",
    "opt = vars(args)\n",
    "print(args.cuda)\n",
    "\n",
    "embedding, opt, vocab = load_meta_with_vocab(opt, gen_name(args.data_dir, args.meta, version, suffix='pick'))\n",
    "\n",
    "print(opt)\n",
    "model = DocReaderModel.load(model_filepath,embedding,gpu=args.cuda)\n",
    "\n",
    "\n",
    "model.setup_eval_embed(embedding)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "\n",
    "with open(os.path.join('resource', 'vocab_tag.pick'),'rb') as f:\n",
    "    vocab_tag = pickle.load(f)\n",
    "with open(os.path.join('resource','vocab_ner.pick'),'rb') as f:\n",
    "    vocab_ner = pickle.load(f)\n",
    "    \n",
    "NLP = spacy.load('en')\n",
    "print('Done loading model and resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding( m, threshold=5):\n",
    "    encoding = np.ones((m, m), dtype=np.float32)\n",
    "    for i in range(m):\n",
    "        for j in range(i, m):\n",
    "            if j - i > threshold:\n",
    "                encoding[i][j] = float(1.0 / math.log(j - i + 1))\n",
    "    return torch.from_numpy(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_encoding(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = BatchGen(gen_name(args.data_dir, args.dev_data, version),\n",
    "                      batch_size=args.batch_size,\n",
    "                      gpu=False, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_squad(model, data, v2_on=False):\n",
    "    data.reset()\n",
    "    span_predictions = {}\n",
    "    label_predictions = {}\n",
    "    for batch in data:\n",
    "        phrase, spans, scores = model.predict(batch)\n",
    "        print(batch)\n",
    "        uids = batch['uids']\n",
    "        for uid, pred in zip(uids, phrase):\n",
    "            span_predictions[uid] = pred\n",
    "        if v2_on:\n",
    "            for uid, pred in zip(uids, scores):\n",
    "                label_predictions[uid] = pred\n",
    "        break\n",
    "    return span_predictions, label_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_squad(model,dev_data,v2_on=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple Inc. 0.0051208297 0.38858845829963684\n",
      "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services.\n",
      "\n",
      "Secondarial 0.9864363 0.41445234417915344\n",
      "The second context is called Secondarial.\n"
     ]
    }
   ],
   "source": [
    "data = [{'context':\"Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, and the HomePod smart speaker. Apple's software includes the macOS and iOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode. Its online services include the iTunes Store, the iOS App Store and Mac App Store, Apple Music, and iCloud. Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell Wozniak's Apple I personal computer. It was incorporated as Apple Computer, Inc., in January 1977, and said to be a tech unicorn.\",\n",
    "         'question':'Wha?',\n",
    "         'uid':'n2iu2b342'},\n",
    "       {'context':'The second context is called Secondarial.  The third is not.',\n",
    "       'question':'What is the second context called?',\n",
    "       'uid':'zus3bui2'}]\n",
    "\n",
    "\n",
    "v2_on = True\n",
    "is_train = False\n",
    "passages = [reform_text(sample['context']) for sample in data]\n",
    "passage_tokened = [doc for doc in NLP.pipe(passages, batch_size=1000, n_threads=3)]\n",
    "\n",
    "question_list = [reform_text(sample['question']) for sample in data]\n",
    "question_tokened = [question for question in NLP.pipe(question_list, batch_size=1000, n_threads=3)]\n",
    "\n",
    "\n",
    "generated_data = []\n",
    "\n",
    "for idx, sample in enumerate(data):\n",
    "    feat_dict = feature_func_eval(sample, question_tokened[idx], passage_tokened[idx], vocab, vocab_tag, vocab_ner, is_train, v2_on)\n",
    "    generated_data.append(feat_dict)\n",
    "\n",
    "pred_data = BatchGen( None,\n",
    "                  batch_size=args.batch_size,\n",
    "                  gpu=False, \n",
    "                 is_train=False, \n",
    "                 data_json=generated_data)\n",
    "\n",
    "\n",
    "top_k = 1\n",
    "for batch in pred_data:\n",
    "    start,end,lab = model.predict_eval(batch)\n",
    "\n",
    "    max_len = model.opt['max_len'] or start.size(1)\n",
    "    doc_len = start.size(1)\n",
    "    pos_enc = model.position_encoding(doc_len, max_len)\n",
    "    \n",
    "    for i,r in enumerate(lab):\n",
    "        scores = torch.ger(start[i], end[i])\n",
    "        scores = scores * pos_enc\n",
    "        scores.triu_()\n",
    "        scores = scores.numpy()\n",
    "        \n",
    "        label_score = float(lab[i])\n",
    "\n",
    "        \n",
    "        for k in range(1,top_k+1):\n",
    "            print()\n",
    "            best_idx = np.argpartition(scores, -k, axis=None)[-k]\n",
    "            best_score = np.partition(scores, -k, axis=None)[-k]\n",
    "            s_idx, e_idx = np.unravel_index(best_idx, scores.shape)\n",
    "            \n",
    "\n",
    "            beginning_index = 0\n",
    "            for z in range(s_idx-1,0,-1):\n",
    "                \n",
    "                cur_tok = passage_tokened[i][z].text\n",
    "                \n",
    "\n",
    "                if cur_tok == '.' or cur_tok == '!' or cur_tok == '?' or cur_tok == '\\n' or cur_tok == ']':\n",
    "                    beginning_index = z+1\n",
    "                    break\n",
    "\n",
    "            end_index = len(passage_tokened[i])\n",
    "            for z in range(e_idx,len(passage_tokened[i])):\n",
    "                \n",
    "                cur_tok = passage_tokened[i][z].text\n",
    "                \n",
    "                \n",
    "                if cur_tok == '.' or cur_tok == '!' or cur_tok == '?' or cur_tok == '\\n' or cur_tok == '[':\n",
    "                    end_index = z+1\n",
    "                    break\n",
    "\n",
    "            snippet = passage_tokened[i][beginning_index:end_index]\n",
    "\n",
    "\n",
    "            if label_score > .5:\n",
    "                print('No Answer :',best_score,label_score)\n",
    "    #             print(passage_tokened[i][s_idx:e_idx+1], best_score, label_score)\n",
    "            else:\n",
    "                print(passage_tokened[i][s_idx:e_idx+1].text, best_score, label_score)\n",
    "                print(snippet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_tokened[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = NLP('My name is Donald Trump and I work at Google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in doc:\n",
    "    print(w.ent_type_, w.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_func(question, context):\n",
    "    counter = Counter(w.text.lower() for w in context)\n",
    "    total = sum(counter.values())\n",
    "    freq = [counter[w.text.lower()] / total for w in context]\n",
    "    question_word = {w.text for w in question}\n",
    "    question_lower = {w.text.lower() for w in question}\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in question}\n",
    "    match_origin = [1 if w in question_word else 0 for w in context]\n",
    "    match_lower = [1 if w.text.lower() in question_lower else 0 for w in context]\n",
    "    match_lemma = [1 if (w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma else 0 for w in context]\n",
    "    features = np.asarray([freq, match_origin, match_lower, match_lemma], dtype=np.float32).T.tolist()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_func(NLP('What is the president?'),NLP('What the president is Donald Trump and he was a man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
